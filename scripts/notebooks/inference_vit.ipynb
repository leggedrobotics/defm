{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d81c0d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2026, ETH Zurich, Manthan Patel\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from defm.utils import preprocess_depth_image\n",
    "\n",
    "# Add the project root (defm) to sys.path\n",
    "root_dir = Path(os.getcwd()).parent.parent.resolve()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "model_list = [\"defm_vit_s14\", \"defm_vit_l14\"]  # Available DeFM ViT models\n",
    "\n",
    "MODEL_NAME = \"defm_vit_l14\"\n",
    "PATCH_SIZE = 14  # ViT patch size\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\n",
    "    repo_or_dir=root_dir,  # Path to your DeFM root folder\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    pretrained=True,\n",
    ")\n",
    "model.eval().to(DEVICE)\n",
    "print(\n",
    "    f\"Loaded model: {MODEL_NAME} with {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Inference Example with Dummy Data\n",
    "dummy_depth = (\n",
    "    torch.randn(224, 224, 1) * 100\n",
    ")  # Dummy depth input with max depth 100 meters\n",
    "\n",
    "# Target Size must be a multiple of the patch size\n",
    "# If target size is None, the input size is resized to the nearest multiple of the patch size\n",
    "# The passed depth image should be in meters\n",
    "# This is very important for correct metric-depth based normalization\n",
    "\n",
    "normalized_depth = preprocess_depth_image(\n",
    "    dummy_depth,\n",
    "    patch_size=PATCH_SIZE,\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.get_intermediate_layers(\n",
    "        normalized_depth, n=1, reshape=True, return_class_token=True\n",
    "    )\n",
    "\n",
    "spatial_tokens = output[0][0]\n",
    "class_token = output[0][1]\n",
    "\n",
    "print(f\"Output Spatial Tokens: {spatial_tokens.shape}\")  # (B, C, H', W')\n",
    "print(f\"Output Class Token: {class_token.shape}\")  # (B, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
