{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09f5157",
   "metadata": {},
   "source": [
    "#### DeFM: Semantic Awareness via PCA Visualization\n",
    "\n",
    "This notebook demonstrates DeFM's ability to extract consistent semantic representations from purely geometric depth data.\n",
    "\n",
    "üõ†Ô∏è Example Scenarios\n",
    "1. Cross-Sensor Stability (Cups): Analyzes household objects captured via Active Stereo (D-435), LiDAR (L-515), and Neural Stereo (ZED X, ZED 2i).\n",
    "\n",
    "2. Robotics in the Wild (Ladder): Depth images from a Realsense D-435 mounted on an ANYmal-D Quadrupedal robot during ladder climbing tasks.\n",
    "\n",
    "3. Monocular Depth Estimator (Drawers): Depth Images generated from RGB images using Depth Anything V2 (DAv2). Notice the consistent features for the drawer handles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2026, ETH Zurich, Manthan Patel\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the project root (defm) to sys.path\n",
    "root_dir = Path(os.getcwd()).parent.resolve() \n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "    \n",
    "from defm.utils import preprocess_depth_dav2, preprocess_depth_image\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"defm_vit_l14\"  # Only ViT models are supported currently\n",
    "PATCH_SIZE = 14          # ViT patch size\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Cup Examples\n",
    "DEPTH_INPUT = \"metric\" # \"metric\" or \"dav2\"\n",
    "H, W = 518, 518      # Evaluation resolution\n",
    "BACKGROUND_THRESHOLD = 0.5\n",
    "INPUT_PATH = root_dir / \"example_images/cups\"\n",
    "DEPTH_MULTIPLIER = 100.0  # Scale saved depth image to convert to meters\n",
    "\n",
    "# 2. Ladder Examples (Realsense D-435 Depth)\n",
    "# DEPTH_INPUT = \"metric\" # \"metric\" or \"dav2\"\n",
    "# H, W = 518, 518      # Evaluation resolution\n",
    "# BACKGROUND_THRESHOLD =  1.0 # Dont remove any background\n",
    "# INPUT_PATH = root_dir / \"example_images/ladders\"\n",
    "# DEPTH_MULTIPLIER = 100.0  # Scale saved depth image to convert to meters\n",
    "\n",
    "# 3. Drawer Examples which use Monocular Depth Estimator Depth (MDE)\n",
    "# DEPTH_INPUT = \"dav2\" # \"metric\" or \"dav2\"\n",
    "# H, W = 700, 700      # Evaluation resolution\n",
    "# BACKGROUND_THRESHOLD = 1.0 # Dont remove any background\n",
    "# INPUT_PATH = root_dir / \"example_images/drawers\"\n",
    "# DEPTH_MULTIPLIER = 1.0  # Scale saved depth image to convert to meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name=MODEL_NAME):\n",
    "    \"\"\"Loads DeFM backbone via TorchHub from local source.\"\"\"\n",
    "    model = torch.hub.load(\n",
    "        repo_or_dir='../', # Adjust path to your root DeFM directory\n",
    "        model=model_name,\n",
    "        source='local',\n",
    "        pretrained=True\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Loaded {model_name} to {DEVICE}\")\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(H, W)):\n",
    "    \n",
    "    # Read image\n",
    "    img_np = cv2.imread(image_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "    img_np = img_np / DEPTH_MULTIPLIER\n",
    "\n",
    "    if DEPTH_INPUT == \"metric\":\n",
    "        depth_tensor = preprocess_depth_image(img_np, target_size=target_size, patch_size=PATCH_SIZE)\n",
    "    elif DEPTH_INPUT == \"dav2\":\n",
    "        depth_tensor = preprocess_depth_dav2(img_np, target_size=target_size, patch_size=PATCH_SIZE)\n",
    "    else:   \n",
    "        raise ValueError(\"DEPTH_INPUT must be either 'metric' or 'dav2'\")\n",
    "    \n",
    "    return depth_tensor.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a388bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch_tokens(model, input_tensor, device):\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = model.get_intermediate_layers(input_tensor, n=1, return_class_token=False, norm=True)\n",
    "        patch_tokens = feats[0].cpu().numpy()  # [1, N, D]\n",
    "    return patch_tokens\n",
    "\n",
    "def run_pca_viz(input_folder, n_components=3):\n",
    "    image_paths = sorted([str(p) for p in Path(input_folder).glob(\"*.png\")])\n",
    "    all_tokens = []\n",
    "    raw_images = []\n",
    "\n",
    "    # First pass: extract patch tokens per image\n",
    "    for img_path in image_paths:\n",
    "        input_tensor = preprocess_image(img_path)\n",
    "        patch_tokens = extract_patch_tokens(model, input_tensor, device)  # [1,N,D]\n",
    "        patch_tokens = patch_tokens.squeeze(0)  # [N,D]\n",
    "        all_tokens.append(patch_tokens)\n",
    "        raw_images.append(np.array(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))[:,:,0])\n",
    "\n",
    "    all_patch_tokens = np.stack(all_tokens)  # [num_images, N, D]\n",
    "    num_images, num_patches, feat_size = all_patch_tokens.shape\n",
    "\n",
    "    # Fit per-image PCA for foreground masks\n",
    "    fg_pca = PCA(n_components=1)\n",
    "    foreground_masks = []\n",
    "    all_patches_flat = all_patch_tokens.reshape(-1, feat_size)\n",
    "    reduced_patches = fg_pca.fit_transform(all_patches_flat)\n",
    "    norm_patches = minmax_scale(reduced_patches).reshape(num_images, num_patches)\n",
    "\n",
    "    # Generate binary masks per image\n",
    "    for i in range(num_images):\n",
    "        mask = norm_patches[i] < BACKGROUND_THRESHOLD # Threshold for foreground\n",
    "        foreground_masks.append(mask)\n",
    "\n",
    "    # Extract only foreground patches across all images\n",
    "    fg_patches = np.vstack([\n",
    "        all_patch_tokens[i][foreground_masks[i]]\n",
    "        for i in range(num_images)\n",
    "    ])\n",
    "\n",
    "    print(f\"Total foreground patches for global PCA: {fg_patches.shape}\")\n",
    "\n",
    "    # Fit global PCA to foreground patches\n",
    "    object_pca = PCA(n_components=n_components)\n",
    "    reduced_fg_patches = object_pca.fit_transform(fg_patches)\n",
    "    reduced_fg_patches = minmax_scale(reduced_fg_patches)\n",
    "\n",
    "    print(\"Explained variance ratio:\", object_pca.explained_variance_ratio_)\n",
    "\n",
    "    # Prepare index slicing for each image\n",
    "    mask_indices = np.cumsum([0] + [np.sum(m) for m in foreground_masks])\n",
    "\n",
    "    # Visualization per image\n",
    "    num_cols = 4  # 2 samples per row, each with PCA + original\n",
    "    num_rows = int(np.ceil(num_images / 2))\n",
    "\n",
    "    plt.figure(figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        patch_image = np.zeros((num_patches, n_components), dtype='float32')\n",
    "        patch_image[foreground_masks[i], :] = reduced_fg_patches[mask_indices[i]:mask_indices[i+1], :]\n",
    "\n",
    "        # Reshape to patch grid (H//PATCH_SIZE, W//PATCH_SIZE, n_components)\n",
    "        color_patches = patch_image.reshape([H // PATCH_SIZE, W // PATCH_SIZE, n_components])\n",
    "\n",
    "        # Compute row/col index for combined figure\n",
    "        row_idx = i // 2\n",
    "        col_offset = (i % 2) * 2  # 0 or 2\n",
    "\n",
    "        # ----- PCA Visualization -----\n",
    "        plt.subplot(num_rows, num_cols, (row_idx * num_cols) + col_offset + 1)\n",
    "        plt.imshow(color_patches)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # ----- Original Image -----\n",
    "        plt.subplot(num_rows, num_cols, (row_idx * num_cols) + col_offset + 2)\n",
    "        image = cv2.resize(raw_images[i], (W, H))\n",
    "        plt.imshow(image, cmap='turbo_r', vmin=0, vmax=255)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Adjust layout and show combined figure\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "# Run the pipeline\n",
    "run_pca_viz(INPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
